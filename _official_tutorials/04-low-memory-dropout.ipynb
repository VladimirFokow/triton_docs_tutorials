{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "# --------\n",
    "#\n",
    "# The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n",
    "# of deep neural networks in low-data regime (i.e. regularization).\n",
    "#\n",
    "# It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n",
    "# output has a probability :math:`p` of being changed to zero and otherwise it is copied from the input.\n",
    "# This forces the network to perform well even when only :math:`1 - p` scalars from the input are available.\n",
    "#\n",
    "# At evaluation time we want to use the full power of the network so we set :math:`p=0`. Naively this would\n",
    "# increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n",
    "# in the output softmax temperature). To prevent this we multiply the output by :math:`\\frac{1}{1 - p}`, which\n",
    "# keeps the norm consistent regardless of the dropout probability.\n",
    "#\n",
    "# Let's first take a look at the baseline implementation.\n",
    "\n",
    "import tabulate\n",
    "import torch\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _dropout(\n",
    "    x_ptr,  # pointer to the input\n",
    "    x_keep_ptr,  # pointer to a mask of 0s and 1s\n",
    "    output_ptr,  # pointer to the output\n",
    "    n_elements,  # number of elements in the `x` tensor\n",
    "    p,  # probability that an element of `x` is changed to zero\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "    # Load data\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n",
    "    # The line below is the crucial part, described in the paragraph above!\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    # Write-back output\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def dropout(x, x_keep, p):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "# Input tensor\n",
    "x = torch.randn(size=(10, ), device=DEVICE)\n",
    "# Dropout mask\n",
    "p = 0.5\n",
    "x_keep = (torch.rand(size=(10, ), device=DEVICE) > p).to(torch.int32)\n",
    "#\n",
    "output = dropout(x, x_keep=x_keep, p=p)\n",
    "print(tabulate.tabulate([\n",
    "    [\"input\"] + x.tolist(),\n",
    "    [\"keep mask\"] + x_keep.tolist(),\n",
    "    [\"output\"] + output.tolist(),\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeded dropout\n",
    "# --------------\n",
    "#\n",
    "# The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n",
    "# we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n",
    "# very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n",
    "# https://pytorch.org/docs/stable/checkpoint.html). In this tutorial we'll describe an alternative implementation\n",
    "# that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n",
    "# of persisting randomness across multiple invocations of the kernel.\n",
    "#\n",
    "# Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n",
    "# :code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n",
    "# values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n",
    "# other :ref:`random number generation strategies<Random Number Generation>`.\n",
    "#\n",
    "# .. note::\n",
    "#    Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).\n",
    "#\n",
    "# Let's put it all together.\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _seeded_dropout(\n",
    "    x_ptr,\n",
    "    output_ptr,\n",
    "    n_elements,\n",
    "    p,\n",
    "    seed,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    # compute memory offsets of elements handled by this instance\n",
    "    pid = tl.program_id(axis=0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    # load data from x\n",
    "    mask = offsets < n_elements\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    # randomly prune it\n",
    "    random = tl.rand(seed, offsets)\n",
    "    x_keep = random > p\n",
    "    # write-back\n",
    "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def seeded_dropout(x, p, seed):\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_contiguous()\n",
    "    n_elements = x.numel()\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
    "    return output\n",
    "\n",
    "\n",
    "x = torch.randn(size=(10, ), device=DEVICE)\n",
    "# Compare this to the baseline - dropout mask is never instantiated!\n",
    "output = seeded_dropout(x, p=0.5, seed=123)\n",
    "output2 = seeded_dropout(x, p=0.5, seed=123)\n",
    "output3 = seeded_dropout(x, p=0.5, seed=512)\n",
    "\n",
    "print(\n",
    "    tabulate.tabulate([\n",
    "        [\"input\"] + x.tolist(),\n",
    "        [\"output (seed = 123)\"] + output.tolist(),\n",
    "        [\"output (seed = 123)\"] + output2.tolist(),\n",
    "        [\"output (seed = 512)\"] + output3.tolist(),\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et Voil√†! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n",
    "# If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n",
    "# to explore the `python/triton/language/random.py`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "# ---------\n",
    "#\n",
    "# 1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n",
    "# 2. Add support for striding.\n",
    "# 3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix on the fly each time using a seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# ----------\n",
    "#\n",
    "# .. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n",
    "# .. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
