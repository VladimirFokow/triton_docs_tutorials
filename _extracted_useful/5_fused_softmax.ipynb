{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb031c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "DEVICE = triton.runtime.driver.active.get_active_torch_device()  # device(type='cuda', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca343d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20776c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Step 1 #########\n",
    "# first we'll look at the naive implementation jic you need a refresher\n",
    "def naive_softmax(x: torch.Tensor) -> torch.Tensor:\n",
    "    '''\n",
    "    Built for input of size (M,N)\n",
    "    Safe softmax is when we subtract the maximum element in order to avoid numerical \n",
    "    overflows when doing .exp(); softmax is invariant to this shift\n",
    "    '''\n",
    "    # read MN elements, find their max along N, and write M elements (the maxes)\n",
    "    x_max = x.max(dim=1)[0] \n",
    "        # pytorch actually outputs a tuple of (values, indices) so [0] grabs the values;\n",
    "        # we ignored the indices when talking about memory writes above\n",
    "    # read MN + M elements, subtraction is MN flops, and write MN elements\n",
    "    z = x - x_max[:, None]\n",
    "    # read MN elements and write MN elemnts\n",
    "    numerator = torch.exp(z)\n",
    "        # exp is actually a lot of flops per element but we're only worried about mem ops rn\n",
    "    # read MN elements, do MN flops to find M sum values, and then write M elements\n",
    "    denominator = numerator.sum(dim=1)\n",
    "    # read MN + M elements, division is MN flops, then write MN elements\n",
    "    out = numerator / denominator[:, None]\n",
    "\n",
    "    # in total we did 8MN + 4M memory operations\n",
    "    # (read 5MN + 2M elements; wrote 3MN + 2M elements)\n",
    "    return out\n",
    "\"\"\"\n",
    "that's a whole lot of memory operations. we'd prefer to have a custom \"fused\" kernel that only \n",
    "reads x from DRAM once and does all the necessary computations on SRAM as opposed to repeatedly \n",
    "reading & writing to DRAM. that would give a ~4x speedup since \n",
    "(8MN + 4M)/2MN = 4 (ignoring the solo M term a la big O notation)\n",
    "\n",
    "torch.jit.script flag and torch.compile actually aim to do this fusion automatically but can't \n",
    "pull it off quite as well as we're about to\n",
    "\n",
    "our fused softmax kernel will work as follows:\n",
    "each program (individual call of the kernel) loads a set of rows of the input matrix X which are\n",
    "  strided by number of programs, softmaxes it and writes back the result to the output Y\n",
    "\n",
    "note an important limitation of Triton is that each block must have a power-of-two number of\n",
    "  elements, so we need to internally \"pad\" each row and guard the memory operations properly\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a501d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0f210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Step 2 #########\n",
    "def test_softmax_kernel(size: tuple, atol=1e-3, rtol=1e-3, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Here is where we test the wrapper function and kernel that we wrote \n",
    "    above to ensure all our values are correct, using pytorch as the \n",
    "    correct answer to compare against\n",
    "\n",
    "    we'll use an irregular number of rows & cols to verify that our padding mechanism works\n",
    "    \"\"\"\n",
    "    # create input data\n",
    "    torch.manual_seed(0)\n",
    "    assert type(size) is tuple and len(size) == 2\n",
    "    x = torch.randn(size[0], size[1], device=DEVICE)\n",
    "    # run kernel & pytorch reference implementation\n",
    "    z_tri = softmax(x)\n",
    "    z_ref = torch.softmax(x, axis=1)\n",
    "        # notice our implementation doesn't give a choice for what axis to softmax along.\n",
    "        # this is a common theme of custom GPU kernels; because pytorch has to write code that\n",
    "        #  is more general, it is slower than it could be\n",
    "    # compare\n",
    "    torch.testing.assert_close(z_tri, z_ref, atol=atol, rtol=rtol)\n",
    "    print(\"PASSED\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c7e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Step 3 #########\n",
    "\"\"\"\n",
    "before we create the wrapper function that enqueues the kernel and its meta-parameters, we're going to\n",
    " fetch the specifications of our GPU to help later when defining our meta-parameters such that they're \n",
    " especially well suited (fast) to the specific GPU we're using\n",
    "\"\"\"\n",
    "# fetching a dictionary full of the GPU's specifications\n",
    "properties = triton.runtime.driver.active.utils.get_device_properties(DEVICE.index)\n",
    "# each Streaming Multi-processor (SM) is like a mini-processor that can run multiple programs\n",
    "NUM_SM = properties[\"multiprocessor_count\"] \n",
    "# registers are the fastest memory on the GPU\n",
    "NUM_REGS = properties[\"max_num_regs\"] \n",
    "    # each SM has a limited number of registers; \n",
    "    # programs share these registers, so using too many per program limits parallelism\n",
    "# each SM has a dedicated pool of SRAM that it can access\n",
    "# since there can be multiple programs per SM, those programs share the same SRAM\n",
    "    # ^that will be very useful information later in the matmul tutorial\n",
    "TOTAL_SRAM_PER_SM = properties[\"max_shared_mem\"] \n",
    "# a warp is a group of threads that execute together\n",
    "# a thread can be thought of as analagous to a single CPU core, but far more limited in the operations it can do\n",
    "WARP_SIZE = properties[\"warpSize\"]# usually 32 on nvidia GPUs and 64 on AMD\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    helper/wrapper function to \n",
    "        1) allocate the output tensor and \n",
    "        2) enque the above kernel with appropriate grid/block sizes\n",
    "    \n",
    "    This wrapper function does not connect us to pytorch's graph, meaning it does not\n",
    "    support backpropogation. That (as well as a backward pass kernel) is for a future lesson\n",
    "    '''\n",
    "    # this kernel is only built to support matrices; expanding that support is simple but for a later lesson\n",
    "    assert x.ndim == 2\n",
    "    n_rows, n_cols = x.shape\n",
    "\n",
    "    # the block size is the smallest power of 2 greater than the number of columns in x\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    # a trick we can use is to ask the compiler to use more threads per row by\n",
    "    #  increasing the number of warps (`num_warps`) over which each row is distributed.\n",
    "    # for now these settings are just a heuristic\n",
    "    # you will see in the next tutorial how to auto-tune this value in a more natural way\n",
    "    #   so you don't have to come up with manual heuristics yourself\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "\n",
    "    # Rather than executing all code within a kernel sequentially, the GPU can actually do multiple things at once.\n",
    "    # This is called the number of software pipelining stages.\n",
    "    # For example, with 2 stages we can have one do the operation while the other is loading the next operands \n",
    "    #  from DRAM into SRAM. With 3 we can have one do current operations, one load next operands, and one saving \n",
    "    #  previous operands.\n",
    "    # Triton just needs the number of stages and it'll handle how to use them efficiently.\n",
    "    # Here we use a simple heuristic of \"if we've got a lot of memory, use 4. otherwise use 2\"\n",
    "    num_stages = 4 if TOTAL_SRAM_PER_SM > 200_000 else 2\n",
    "\n",
    "    # allocate output\n",
    "    y = torch.empty_like(x)\n",
    "\n",
    "    # .warmup() pre-compiles kernel and tells us how many registers and how much shared memory it needs\n",
    "    kernel = _softmax_kernel.warmup(x, y, # this warmup depends on the attributes of the input and output\n",
    "                                    x.stride(0), y.stride(0), # see below\n",
    "                                    n_rows, n_cols,\n",
    "                                    BLOCK_SIZE=BLOCK_SIZE,\n",
    "                                    num_stages=num_stages,\n",
    "                                    num_warps=num_warps,\n",
    "                                    grid=(1,))\n",
    "    # x.stride() for each dimension tells us how many entries in memory a pointer needs to move forward in order\n",
    "    #  to get to the next element of the tensor along the specified dimension. \n",
    "    # For any tensor x that is \"contiguous\", meaning ~cleanly/simply~ defined in memory and for a shape (M, N, K) \n",
    "    #  you can expect x.shape(0) == N*K, x.shape(1)==K, and x.shape(2)==1, or more generally \n",
    "    #  x.shape(-Z)==math.prod(x.shape[-Z:])\n",
    "    # A tensor might be non-contiguous if, for example, it's been saved to memory using torch.view() or some similar\n",
    "    #  operation that leaves the original data in place but messes with dimensions\n",
    "\n",
    "    # here's the info that warmup process gave us\n",
    "    kernel._init_handles()\n",
    "    n_regs = kernel.n_regs\n",
    "    sram_needed_per_program = kernel.metadata.shared \n",
    "\n",
    "    # and here's how we use that info to setup our kernel\n",
    "    # register-based occupancy\n",
    "    reg_occupancy = NUM_REGS // (n_regs * WARP_SIZE * num_warps)\n",
    "        # each SM has NUM_REGS registers (eg 65536)\n",
    "        # each program uses\n",
    "            # n_regs per register thread (eg 32)\n",
    "            # WARP_SIZE threads per warp (32 on Nvidia, 64 on AMD)\n",
    "            # num_warps warps per program (4, 8, or 16 in our case with the aforementioned heuristic)\n",
    "        # so each program needs n_regs * WARP_SIZE * num_warps registers total\n",
    "        # therefore we can fit reg_occupancy programs per SM\n",
    "        # ex. 65536 // (32 * 32 * 8) = 8 programs per SM (assuming num_warps=8)\n",
    "    # shared memory-based occupancy\n",
    "    sram_occupancy = TOTAL_SRAM_PER_SM // sram_needed_per_program\n",
    "    # determines how many programs can run per SM based on register usage and shared memory usage\n",
    "    programs_per_sm = min(reg_occupancy, sram_occupancy)\n",
    "        # the former is the optimal allocation assuming we have more than enough SRAM\n",
    "        # the latter is our limit on SRAM when splitting it equally among all SMs\n",
    "    # then given our number of SMs, we calculate how many programs to run in total\n",
    "    num_programs = min(NUM_SM * programs_per_sm, n_rows)\n",
    "        # ofc we have another limit since we've got no need to surpass the n_rows in the matrix\n",
    "\n",
    "    # grid configuration; each row gets its own program\n",
    "    grid = (num_programs, 1, 1)\n",
    "        # the extra 1's are usually not necessary if they're not being used\n",
    "        # we use them here because the .warmup() we used earlier has a weird quirk in the way\n",
    "        #  it's implemented that forces only 3D launch grids to be inputted once it's been used\n",
    "        # in future lessons we don't use .warmup() so we'll not be required to do this again\n",
    "\n",
    "    # And now we get to run the kernel with our heuristics-based launch grid\n",
    "    kernel[grid](\n",
    "        x, y,\n",
    "        x.stride(0), y.stride(0),\n",
    "        n_rows, n_cols,\n",
    "    )\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5f28d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ab5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Step 4 #########\n",
    "@triton.jit \n",
    "def _softmax_kernel(\n",
    "    input_ptr, output_ptr,\n",
    "    input_row_stride, output_row_stride,    # number of elements to skip when moving to next row\n",
    "    n_rows, n_cols,                         # matrix dimensions\n",
    "    BLOCK_SIZE: tl.constexpr,               # lowest power-of-2 greater than n_cols\n",
    "    num_stages: tl.constexpr,\n",
    "): \n",
    "    # the row that this program starts with is defined by the pid\n",
    "    row_start = tl.program_id(0) \n",
    "    # then this gets the total number of parallel programs, which we'll use to know how large \n",
    "    #  of a step to make in our for loop once we finish the first row\n",
    "    row_step = tl.num_programs(0) \n",
    "        # Each program processes rows strided by row_step \n",
    "        # (ex. if there are 4 programs, program 0 handles rows 0,4,8...)\n",
    "    \n",
    "    # whereas tl.arange() provides an array of values, tl.range() acts as an iterator\n",
    "    for row_idx in tl.range(row_start, n_rows, row_step, num_stages=num_stages):\n",
    "        # rather than actually implement each iteration of the for loop sequentially, triton can use\n",
    "        #  num_stages to work on different interations of the for loop simultaneously. Of course\n",
    "        #  only do this when the iterations don't depend on each other\n",
    "        \n",
    "        # the stride represents how much we need to increase the pointer to advance 1 row\n",
    "        row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "            # inyuiyively input_row_stride should be 1 as long as the input tensor is contiguous.\n",
    "            #  but what if a non-contiguous view of a manipulated tensor were passed in? then\n",
    "            #  input_row_stride matters\n",
    "\n",
    "        # load the row into SRAM, using a mask since BLOCK_SIZE is > than n_cols if n_cols is not a power of 2\n",
    "        col_offsets = tl.arange(0, BLOCK_SIZE) # we can fit each row in a single block\n",
    "        input_ptrs = row_start_ptr + col_offsets\n",
    "        mask = col_offsets < n_cols\n",
    "        row = tl.load(input_ptrs, mask=mask, other=float('-inf')) \n",
    "            # we fill in masked out indices with -inf since that's the value that won't influence softmax\n",
    "\n",
    "        # subtract maximum for numerical stability\n",
    "        row_minus_max = row - tl.max(row, axis=0)\n",
    "            # all the invalid -inf values remain -inf when we subtract the max\n",
    "        # note that exponentiation in Triton is fast but approximate; later we'll learn an even faster alternative\n",
    "        numerator = tl.exp(row_minus_max)\n",
    "            # all the -inf values get set to 0 since exp(-inf)=0\n",
    "        denominator = tl.sum(numerator, axis=0)\n",
    "            # all the invalid 0 values do get summed but don't matter since they're 0\n",
    "        softmax_output = numerator / denominator\n",
    "            # all the invalid 0's are 0/sum and therefore remain 0\n",
    "\n",
    "        # write output back to DRAM\n",
    "        output_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "        tl.store(output_row_start_ptr + col_offsets, softmax_output, mask=mask)\n",
    "            # using our mask we only store back the valid n_cols values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a74f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36feb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######### Step 5 #########\n",
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['N'],\n",
    "        x_vals=[128 * i for i in range(2, 100)],\n",
    "        line_arg='provider',\n",
    "        line_vals=['triton', 'torch'],\n",
    "        line_names=[\"Triton\", \"Torch\"],\n",
    "        styles=[('blue', '-'), ('green', '-')],\n",
    "        ylabel=\"GB/s\",\n",
    "        plot_name=\"softmax-performance\",\n",
    "        args={'M': 4096} # values for function arguments not in x_names\n",
    "    ))\n",
    "def benchmark(M, N, provider):\n",
    "    # making the input data\n",
    "    x = torch.randn(M, N, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "    # these two lines ensure more accurate benchmarks; i usually forget to use them but it's not a big deal\n",
    "    stream = getattr(torch, DEVICE.type).Stream()\n",
    "    getattr(torch, DEVICE.type).set_stream(stream)\n",
    "\n",
    "    if provider == 'torch':\n",
    "        ms = triton.testing.do_bench(lambda: torch.softmax(x, axis=-1))\n",
    "    if provider == 'triton':\n",
    "        ms = triton.testing.do_bench(lambda: softmax(x))\n",
    "    gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "        # 2 = number of memory operations (1 read + 1 write)\n",
    "        # x.numel() = number of elements\n",
    "        # x.element_size() = bytes per element (4 for float32)\n",
    "        # 1e-9 converts bytes to GB\n",
    "        # 1e-3 converts milliseconds to seconds\n",
    "    return gbps(ms)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # always run unit-tests\n",
    "    test_softmax_kernel(size=(1823, 781))\n",
    "\n",
    "    # Only run benchmark if explicitly requested\n",
    "    import sys\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--benchmark\":\n",
    "        benchmark.run(save_path='.', print_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b82abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d1ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ea2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
